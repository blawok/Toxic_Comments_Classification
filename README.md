# Toxic_Comments_Classification

In this project we analyzed toxic comment dataset from Kaggle challenge - https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview. Our goal was to implement model thatâ€™s capable of detecting different types of of toxicity (like threats, obscenity, insults, and identity-based) in comments.

*Disclaimer: the dataset contains text that may be considered profane, vulgar, or highly offensive.*

We perform EDA and modelling of BiGRU in [this notebook](https://github.com/blawok/Toxic_Comments_Classification/blob/master/toxic_comments.ipynb).

However, recently I came across a great example of using BERT for this problem. Code for it can be found [here](https://github.com/blawok/Toxic_Comments_Classification/blob/master/toxic_cnn_bert.ipynb)
